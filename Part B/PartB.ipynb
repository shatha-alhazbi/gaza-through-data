{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocessing"
      ],
      "metadata": {
        "id": "cnpwT4Uie2P9"
      },
      "id": "cnpwT4Uie2P9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43530994-53d2-4d35-9cbf-afbbb0bed939",
      "metadata": {
        "id": "43530994-53d2-4d35-9cbf-afbbb0bed939"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datascience import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Converting File Format\n"
      ],
      "metadata": {
        "id": "lNGX1XxGe_Jg"
      },
      "id": "lNGX1XxGe_Jg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2b46332-65b5-4fda-a462-ddffc646fb88",
      "metadata": {
        "id": "c2b46332-65b5-4fda-a462-ddffc646fb88",
        "outputId": "41c7a45d-e658-49fa-b141-1442bfba1b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /opt/anaconda3/lib/python3.12/site-packages (from pdfplumber) (10.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pdfminer.six==20250327->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pdfminer.six==20250327->pdfplumber) (43.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.21)\n",
            "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "Downloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbbbae08-b67a-4bb0-9ef0-475a9c7cbad8",
      "metadata": {
        "id": "bbbbae08-b67a-4bb0-9ef0-475a9c7cbad8",
        "outputId": "95a4da3c-73f7-453d-c301-3d50c9f0fc69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing pages 1 to 50...\n",
            "Processing pages 51 to 100...\n",
            "Processing pages 101 to 150...\n",
            "Processing pages 151 to 200...\n",
            "Processing pages 201 to 250...\n",
            "Processing pages 251 to 300...\n",
            "Processing pages 301 to 350...\n",
            "Processing pages 351 to 400...\n",
            "Processing pages 401 to 450...\n",
            "Processing pages 451 to 500...\n",
            "Processing pages 501 to 550...\n",
            "Processing pages 551 to 600...\n",
            "Processing pages 601 to 650...\n",
            "Processing pages 651 to 700...\n",
            "Processing pages 701 to 750...\n",
            "Processing pages 751 to 800...\n",
            "Processing pages 801 to 850...\n",
            "Processing pages 851 to 900...\n",
            "Processing pages 901 to 950...\n",
            "Processing pages 951 to 981...\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "# To ignore the warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
        "\n",
        "with pdfplumber.open('killed-in-gaza_moh_2025-03-23.pdf') as pdf:\n",
        "    total_pages = len(pdf.pages)\n",
        "    batch_size = 50  # process 50 pages at a time\n",
        "    batch_number = 1\n",
        "\n",
        "    for start_page in range(0, total_pages, batch_size):\n",
        "        end_page = min(start_page + batch_size, total_pages)\n",
        "        print(f\"Processing pages {start_page+1} to {end_page}...\")\n",
        "\n",
        "        batch_rows = []\n",
        "        columns = None\n",
        "\n",
        "        for page_num in range(start_page, end_page):\n",
        "            page = pdf.pages[page_num]\n",
        "            table = page.extract_table()\n",
        "            if table:\n",
        "                if columns is None:\n",
        "                    columns = table[0]  # first page -> header\n",
        "                    batch_rows.extend(table[1:])  # first page -> skip header\n",
        "                else:\n",
        "                    batch_rows.extend(table)  # other pages -> take all rows\n",
        "\n",
        "        if batch_rows:\n",
        "            # Building a small Table for this batch\n",
        "            batch_table = Table().with_columns(*[(col, []) for col in columns])\n",
        "\n",
        "            for row in batch_rows:\n",
        "                batch_table = batch_table.with_row(row)\n",
        "\n",
        "            # Saving this batch separately\n",
        "            filename = f'killed_in_gaza_batch_{batch_number}.csv'\n",
        "            batch_table.to_csv(filename)\n",
        "\n",
        "        batch_number += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ae262c-c7ca-4c97-b137-8aaf952e1932",
      "metadata": {
        "id": "f9ae262c-c7ca-4c97-b137-8aaf952e1932"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Finding all CSV files\n",
        "csv_files = sorted(glob.glob('killed_in_gaza_batch_*.csv'))\n",
        "\n",
        "# Creating a file to store the combined output\n",
        "with open('killed_in_gaza.csv', 'w', encoding='utf-8-sig', newline='') as output_file:\n",
        "    writer = None\n",
        "\n",
        "    for filename in csv_files:\n",
        "        with open(filename, 'r', encoding='utf-8-sig') as f:\n",
        "            reader = csv.reader(f)\n",
        "\n",
        "            if writer is None:\n",
        "                headers = next(reader)  # read header\n",
        "                writer = csv.writer(output_file)\n",
        "                writer.writerow(headers)\n",
        "\n",
        "            for row in reader:\n",
        "                writer.writerow(row)\n",
        "        os.remove(filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f83a9d-df79-4051-ba97-0ba4050ce4ec",
      "metadata": {
        "id": "17f83a9d-df79-4051-ba97-0ba4050ce4ec"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Loading JSON\n",
        "with open('infrastructure-damaged_2025-04-12.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Flattening records\n",
        "flattened_data = []\n",
        "\n",
        "for record in data:\n",
        "    flat_record = {}\n",
        "    for key, value in record.items():\n",
        "        if isinstance(value, dict):\n",
        "            for sub_key, sub_value in value.items():\n",
        "                flat_record[f\"{key}_{sub_key}\"] = sub_value\n",
        "        else:\n",
        "            flat_record[key] = value\n",
        "    flattened_data.append(flat_record)\n",
        "\n",
        "# Finding all possible headers from all records\n",
        "all_keys = set()\n",
        "for row in flattened_data:\n",
        "    all_keys.update(row.keys())\n",
        "\n",
        "# headers = sorted(all_keys)\n",
        "\n",
        "# Saving to CSV\n",
        "with open('infrastructure_damaged.csv', 'w', newline='', encoding='utf-8-sig') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=headers)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for row in flattened_data:\n",
        "        # filling missing keys with 0\n",
        "        complete_row = {key: row.get(key, 0) for key in headers}\n",
        "        writer.writerow(complete_row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Datasets"
      ],
      "metadata": {
        "id": "8pcCsHH5gHTr"
      },
      "id": "8pcCsHH5gHTr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0fa826-6ce1-4aac-9435-4d8e1946b79c",
      "metadata": {
        "id": "3c0fa826-6ce1-4aac-9435-4d8e1946b79c",
        "outputId": "06faa230-f31a-4178-9e7a-091f7d14aefc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>report_date</th> <th>report_source</th> <th>report_period</th> <th>ext_massacres_cum</th> <th>killed</th> <th>killed_cum</th> <th>ext_killed</th> <th>ext_killed_cum</th> <th>ext_killed_children_cum</th> <th>ext_killed_women_cum</th> <th>injured_cum</th> <th>ext_injured</th> <th>ext_injured_cum</th> <th>ext_civdef_killed_cum</th> <th>med_killed_cum</th> <th>ext_med_killed_cum</th> <th>press_killed_cum</th> <th>ext_press_killed_cum</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>2023-10-07 </td> <td>mohtel       </td> <td>24           </td> <td>0                </td> <td>232   </td> <td>232       </td> <td>232       </td> <td>232           </td> <td>0                      </td> <td>0                   </td> <td>1610       </td> <td>1610       </td> <td>1610           </td> <td>0                    </td> <td>6             </td> <td>6                 </td> <td>1               </td> <td>1                   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-08 </td> <td>mohtel       </td> <td>24           </td> <td>0                </td> <td>138   </td> <td>370       </td> <td>138       </td> <td>370           </td> <td>78                     </td> <td>41                  </td> <td>1788       </td> <td>178        </td> <td>1788           </td> <td>0                    </td> <td>nan           </td> <td>6                 </td> <td>1               </td> <td>1                   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-09 </td> <td>mohtel       </td> <td>24           </td> <td>8                </td> <td>190   </td> <td>560       </td> <td>190       </td> <td>560           </td> <td>91                     </td> <td>61                  </td> <td>2271       </td> <td>483        </td> <td>2271           </td> <td>0                    </td> <td>6             </td> <td>6                 </td> <td>3               </td> <td>3                   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-10 </td> <td>mohtel       </td> <td>24           </td> <td>8                </td> <td>340   </td> <td>900       </td> <td>340       </td> <td>900           </td> <td>260                    </td> <td>230                 </td> <td>4000       </td> <td>1729       </td> <td>4000           </td> <td>0                    </td> <td>nan           </td> <td>6                 </td> <td>7               </td> <td>7                   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-11 </td> <td>gmotel       </td> <td>24           </td> <td>23               </td> <td>200   </td> <td>1100      </td> <td>200       </td> <td>1100          </td> <td>398                    </td> <td>230                 </td> <td>5184       </td> <td>1184       </td> <td>5184           </td> <td>0                    </td> <td>10            </td> <td>10                </td> <td>nan             </td> <td>7                   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-12 </td> <td>mohtel       </td> <td>24           </td> <td>23               </td> <td>317   </td> <td>1417      </td> <td>317       </td> <td>1417          </td> <td>500                    </td> <td>276                 </td> <td>5763       </td> <td>579        </td> <td>5763           </td> <td>0                    </td> <td>11            </td> <td>11                </td> <td>nan             </td> <td>7                   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-13 </td> <td>missing      </td> <td>24           </td> <td>23               </td> <td>483   </td> <td>1900      </td> <td>483       </td> <td>1900          </td> <td>500                    </td> <td>392                 </td> <td>nan        </td> <td>1475       </td> <td>7238           </td> <td>0                    </td> <td>11            </td> <td>11                </td> <td>nan             </td> <td>7                   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-14 </td> <td>gmotel       </td> <td>24           </td> <td>23               </td> <td>328   </td> <td>2228      </td> <td>328       </td> <td>2228          </td> <td>600                    </td> <td>600                 </td> <td>8714       </td> <td>1476       </td> <td>8714           </td> <td>0                    </td> <td>11            </td> <td>11                </td> <td>nan             </td> <td>7                   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-15 </td> <td>gmotel       </td> <td>24           </td> <td>23               </td> <td>442   </td> <td>2670      </td> <td>442       </td> <td>2670          </td> <td>700                    </td> <td>735                 </td> <td>9200       </td> <td>486        </td> <td>9200           </td> <td>0                    </td> <td>nan           </td> <td>11                </td> <td>nan             </td> <td>7                   </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-16 </td> <td>gmotel       </td> <td>24           </td> <td>371              </td> <td>138   </td> <td>2808      </td> <td>138       </td> <td>2808          </td> <td>853                    </td> <td>936                 </td> <td>10850      </td> <td>1650       </td> <td>10850          </td> <td>7                    </td> <td>37            </td> <td>37                </td> <td>nan             </td> <td>7                   </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>\n",
              "<p>... (541 rows omitted)</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "daily_casualties_gaza = Table.read_table('casualties_daily_gaza_2025-04-12.csv')\n",
        "daily_casualties_gaza.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3c0511-3c3d-4de5-bd15-4dacb1be7344",
      "metadata": {
        "id": "2d3c0511-3c3d-4de5-bd15-4dacb1be7344",
        "outputId": "4efa04d3-de6f-4922-fc19-e95c559d127e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>report_date</th> <th>verified.killed</th> <th>verified.killed_cum</th> <th>verified.injured</th> <th>verified.injured_cum</th> <th>verified.killed_children</th> <th>verified.killed_children_cum</th> <th>verified.injured_children</th> <th>verified.injured_children_cum</th> <th>killed_cum</th> <th>killed_children_cum</th> <th>injured_cum</th> <th>injured_children_cum</th> <th>settler_attacks_cum</th> <th>flash_source</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>2023-10-07 </td> <td>6              </td> <td>6                  </td> <td>197             </td> <td>197                 </td> <td>2                       </td> <td>2                           </td> <td>31                       </td> <td>31                           </td> <td>3         </td> <td>0                  </td> <td>23         </td> <td>1                   </td> <td>3                  </td> <td>un          </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-08 </td> <td>8              </td> <td>14                 </td> <td>127             </td> <td>324                 </td> <td>3                       </td> <td>5                           </td> <td>7                        </td> <td>38                           </td> <td>13        </td> <td>1                  </td> <td>194        </td> <td>1                   </td> <td>9                  </td> <td>un          </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-09 </td> <td>4              </td> <td>18                 </td> <td>88              </td> <td>412                 </td> <td>0                       </td> <td>5                           </td> <td>19                       </td> <td>57                           </td> <td>15        </td> <td>3                  </td> <td>295        </td> <td>34                  </td> <td>19                 </td> <td>un          </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-10 </td> <td>5              </td> <td>23                 </td> <td>72              </td> <td>484                 </td> <td>0                       </td> <td>5                           </td> <td>7                        </td> <td>64                           </td> <td>19        </td> <td>3                  </td> <td>332        </td> <td>39                  </td> <td>29                 </td> <td>un          </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-11 </td> <td>6              </td> <td>29                 </td> <td>85              </td> <td>569                 </td> <td>2                       </td> <td>7                           </td> <td>17                       </td> <td>81                           </td> <td>26        </td> <td>3                  </td> <td>427        </td> <td>62                  </td> <td>39                 </td> <td>un          </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>\n",
              "<p>... (546 rows omitted)</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "daily_casualties_westbank = Table.read_table('casualties_daily_west_bank_2025-04-12.csv')\n",
        "daily_casualties_westbank.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a1ae3cb-53e7-4ea5-a767-1fe3d3cb61a4",
      "metadata": {
        "id": "8a1ae3cb-53e7-4ea5-a767-1fe3d3cb61a4",
        "outputId": "3168e7c6-8992-4110-fee7-249a7b3c8f4d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>Index</th> <th>Name</th> <th>مس#ا</th> <th>Born</th> <th>Age</th> <th>Sex</th> <th>ID number</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>1    </td> <td>Mohammed Hani Mohammed Al-Zahhar</td> <td>راهزلا دمحم يناه دمحم    </td> <td>2023-08-09</td> <td>0   </td> <td>m   </td> <td>444196471</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2    </td> <td>Nour Mohammed Aed Udwan         </td> <td>ناودع دئاع دمحم رون      </td> <td>2023-02-27</td> <td>0   </td> <td>f   </td> <td>444484729</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>3    </td> <td>Khaled Mumin Amin Shabir        </td> <td>ريبش 6ما نمؤم دلاخ       </td> <td>2023-04-16</td> <td>0   </td> <td>m   </td> <td>444799647</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>4    </td> <td>Yamen Ibrahim Farid Qaddoura    </td> <td>هرودق ديرف ميهاربا نماي  </td> <td>2023-06-10</td> <td>0   </td> <td>m   </td> <td>445958408</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>5    </td> <td>Ahmed Rami Ahmed Abdel Qadir    </td> <td>رداقلا دبع دمحأ يمار دمحأ</td> <td>2022-12-22</td> <td>0   </td> <td>m   </td> <td>445808546</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>\n",
              "<p>... (50015 rows omitted)</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "killed_in_gaza = Table.read_table('killed_in_gaza.csv')\n",
        "# killed_in_gaza.sort(\"Index\", descending=True).show(5)\n",
        "killed_in_gaza.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef2edaa-adfe-4753-bb32-b5f597bd31f3",
      "metadata": {
        "id": "6ef2edaa-adfe-4753-bb32-b5f597bd31f3",
        "outputId": "9e3a6a18-e09e-43b5-c456-38c49006d4f6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>report_date</th> <th>civic_buildings_destroyed</th> <th>civic_buildings_ext_destroyed</th> <th>educational_buildings_damaged</th> <th>educational_buildings_destroyed</th> <th>educational_buildings_ext_damaged</th> <th>educational_buildings_ext_destroyed</th> <th>places_of_worship_churches_destroyed</th> <th>places_of_worship_ext_churches_destroyed</th> <th>places_of_worship_ext_mosques_damaged</th> <th>places_of_worship_ext_mosques_destroyed</th> <th>places_of_worship_mosques_damaged</th> <th>places_of_worship_mosques_destroyed</th> <th>residential_destroyed</th> <th>residential_ext_destroyed</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>2023-10-07 </td> <td>0                        </td> <td>5                            </td> <td>0                            </td> <td>0                              </td> <td>15                               </td> <td>1                                  </td> <td>0                                   </td> <td>0                                       </td> <td>4                                    </td> <td>2                                      </td> <td>0                                </td> <td>0                                  </td> <td>0                    </td> <td>80                       </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-08 </td> <td>0                        </td> <td>11                           </td> <td>0                            </td> <td>0                              </td> <td>30                               </td> <td>1                                  </td> <td>0                                   </td> <td>0                                       </td> <td>8                                    </td> <td>4                                      </td> <td>0                                </td> <td>0                                  </td> <td>159                  </td> <td>159                      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-09 </td> <td>0                        </td> <td>16                           </td> <td>0                            </td> <td>0                              </td> <td>45                               </td> <td>2                                  </td> <td>0                                   </td> <td>0                                       </td> <td>12                                   </td> <td>6                                      </td> <td>0                                </td> <td>0                                  </td> <td>790                  </td> <td>790                      </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-10 </td> <td>0                        </td> <td>22                           </td> <td>0                            </td> <td>0                              </td> <td>60                               </td> <td>2                                  </td> <td>0                                   </td> <td>0                                       </td> <td>17                                   </td> <td>8                                      </td> <td>0                                </td> <td>0                                  </td> <td>1009                 </td> <td>1009                     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2023-10-11 </td> <td>27                       </td> <td>27                           </td> <td>75                           </td> <td>3                              </td> <td>75                               </td> <td>3                                  </td> <td>0                                   </td> <td>0                                       </td> <td>21                                   </td> <td>10                                     </td> <td>0                                </td> <td>10                                 </td> <td>2835                 </td> <td>2835                     </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>\n",
              "<p>... (529 rows omitted)</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "infrastructure_damaged = Table.read_table('infrastructure_damaged.csv')\n",
        "\n",
        "# Making report_date the first column\n",
        "labels = list(infrastructure_damaged.labels)\n",
        "labels.remove('report_date')\n",
        "new_order = ['report_date'] + labels  # Put it first\n",
        "\n",
        "# Rearrange the Table\n",
        "infrastructure_damaged = infrastructure_damaged.select(new_order)\n",
        "\n",
        "infrastructure_damaged.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Data Quality Assessment\n",
        "\n",
        "Before analyzing our datasets, we need to understand their quality and identify any issues that might affect our analysis.\n",
        "We'll examine all columns in each dataset to identify missing values and understand patterns in the data.\n",
        "\n",
        "Because of the challenging conditions under which this data is collected, we expect to see some gaps. By understanding these gaps, we can make appropriate cleaning decisions that preserve the integrity of the data."
      ],
      "metadata": {
        "id": "Ap3e7P1kgOub"
      },
      "id": "Ap3e7P1kgOub"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c61da0-58ce-40a6-98e3-1f68de7c7aac",
      "metadata": {
        "id": "b1c61da0-58ce-40a6-98e3-1f68de7c7aac"
      },
      "outputs": [],
      "source": [
        "def check_dataset_quality(table, dataset_name):\n",
        "    \"\"\"\n",
        "    Checks the quality of an entire datascience.Table dataset.\n",
        "    table: A datascience.Table object\n",
        "    dataset_name: String with the name of the dataset for reporting\n",
        "    \"\"\"\n",
        "    print(f\"\\n---------- CHECKING QUALITY OF {dataset_name} ----------\")\n",
        "\n",
        "    #create a results table\n",
        "    results = Table().with_columns(\n",
        "        'Column Name', [],\n",
        "        'Data Type', [],\n",
        "        'Count', [],\n",
        "        'Num Missing', [],\n",
        "        'Percent Missing', [],\n",
        "        'Num Unique Values', [],\n",
        "        'Min Value', [],\n",
        "        'Max Value', []\n",
        "    )\n",
        "\n",
        "    num_rows = table.num_rows\n",
        "    print(f\"Total rows: {num_rows}\")\n",
        "\n",
        "    for col_name in table.labels:\n",
        "        print(f\"\\nChecking column: {col_name}\")\n",
        "\n",
        "        #extract column as an array\n",
        "        col_values = table.column(col_name)\n",
        "\n",
        "        try:\n",
        "            example_value = col_values.item(0) if num_rows > 0 else None\n",
        "            data_type = type(example_value).__name__\n",
        "        except:\n",
        "            data_type = \"unknown\"\n",
        "\n",
        "        #count missing/none values\n",
        "        none_count = 0\n",
        "        try:\n",
        "            none_count = table.where(col_name, are.equal_to(None)).num_rows\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        #np.nan\n",
        "        nan_count = 0\n",
        "        try:\n",
        "            #for the numeric columns, check if values are NaN\n",
        "            if data_type in ('int', 'float'):\n",
        "                def is_nan(x):\n",
        "                    try:\n",
        "                        return np.isnan(x)\n",
        "                    except:\n",
        "                        return False\n",
        "\n",
        "                # Apply the function to count NaNs\n",
        "                nan_array = table.apply(is_nan, col_name)\n",
        "                nan_count = sum(nan_array)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        #count empty strings for text columns\n",
        "        empty_count = 0\n",
        "        try:\n",
        "            if data_type == 'str':\n",
        "                empty_count = table.where(col_name, are.equal_to('')).num_rows\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        #total missing\n",
        "        total_missing = none_count + nan_count + empty_count\n",
        "        percent_missing = (total_missing / num_rows * 100) if num_rows > 0 else 0\n",
        "\n",
        "        #unique values\n",
        "        unique_count = 0\n",
        "        try:\n",
        "            #group by the column and count unique values\n",
        "            unique_count = table.group(col_name).num_rows\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        #get min and max just to see if any unuslally small or large vals\n",
        "        min_value = \"N/A\"\n",
        "        max_value = \"N/A\"\n",
        "        try:\n",
        "            if data_type in ('int', 'float'):\n",
        "                valid_values = table.where(col_name, are.not_equal_to(None)).column(col_name)\n",
        "                if len(valid_values) > 0:\n",
        "                    min_value = min(valid_values)\n",
        "                    max_value = max(valid_values)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        results = results.with_row([\n",
        "            col_name,\n",
        "            data_type,\n",
        "            num_rows,\n",
        "            total_missing,\n",
        "            round(percent_missing, 2),\n",
        "            unique_count,\n",
        "            min_value,\n",
        "            max_value\n",
        "        ])\n",
        "\n",
        "    #print a summary\n",
        "    problem_cols = results.where('Percent Missing', are.above(0))\n",
        "\n",
        "    if problem_cols.num_rows > 0:\n",
        "        print(\"\\n----- SUMMARY OF DATA QUALITY ISSUES -----\")\n",
        "        print(f\"Found {problem_cols.num_rows} columns with missing values:\")\n",
        "        problem_cols.select('Column Name', 'Percent Missing').show()\n",
        "    else:\n",
        "        print(\"\\nNo missing values found in any columns!\")\n",
        "\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095cbc35-e402-4e00-8a44-84266899a079",
      "metadata": {
        "id": "095cbc35-e402-4e00-8a44-84266899a079",
        "outputId": "13708dea-6753-4660-ad87-4a77d5b5d7b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- CHECKING QUALITY OF GAZA CASUALTIES ----------\n",
            "Total rows: 551\n",
            "\n",
            "Checking column: report_date\n",
            "\n",
            "Checking column: report_source\n",
            "\n",
            "Checking column: report_period\n",
            "\n",
            "Checking column: ext_massacres_cum\n",
            "\n",
            "Checking column: killed\n",
            "\n",
            "Checking column: killed_cum\n",
            "\n",
            "Checking column: ext_killed\n",
            "\n",
            "Checking column: ext_killed_cum\n",
            "\n",
            "Checking column: ext_killed_children_cum\n",
            "\n",
            "Checking column: ext_killed_women_cum\n",
            "\n",
            "Checking column: injured_cum\n",
            "\n",
            "Checking column: ext_injured\n",
            "\n",
            "Checking column: ext_injured_cum\n",
            "\n",
            "Checking column: ext_civdef_killed_cum\n",
            "\n",
            "Checking column: med_killed_cum\n",
            "\n",
            "Checking column: ext_med_killed_cum\n",
            "\n",
            "Checking column: press_killed_cum\n",
            "\n",
            "Checking column: ext_press_killed_cum\n",
            "\n",
            "----- SUMMARY OF DATA QUALITY ISSUES -----\n",
            "Found 5 columns with missing values:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>Column Name</th> <th>Percent Missing</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>killed          </td> <td>4.9            </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>killed_cum      </td> <td>1.27           </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>injured_cum     </td> <td>2.72           </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>med_killed_cum  </td> <td>75.68          </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>press_killed_cum</td> <td>74.23          </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- CHECKING QUALITY OF WEST BANK CASUALTIES ----------\n",
            "Total rows: 551\n",
            "\n",
            "Checking column: report_date\n",
            "\n",
            "Checking column: verified.killed\n",
            "\n",
            "Checking column: verified.killed_cum\n",
            "\n",
            "Checking column: verified.injured\n",
            "\n",
            "Checking column: verified.injured_cum\n",
            "\n",
            "Checking column: verified.killed_children\n",
            "\n",
            "Checking column: verified.killed_children_cum\n",
            "\n",
            "Checking column: verified.injured_children\n",
            "\n",
            "Checking column: verified.injured_children_cum\n",
            "\n",
            "Checking column: killed_cum\n",
            "\n",
            "Checking column: killed_children_cum\n",
            "\n",
            "Checking column: injured_cum\n",
            "\n",
            "Checking column: injured_children_cum\n",
            "\n",
            "Checking column: settler_attacks_cum\n",
            "\n",
            "Checking column: flash_source\n",
            "\n",
            "----- SUMMARY OF DATA QUALITY ISSUES -----\n",
            "Found 8 columns with missing values:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>Column Name</th> <th>Percent Missing</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>verified.killed              </td> <td>26.86          </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>verified.killed_cum          </td> <td>26.68          </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>verified.injured             </td> <td>29.4           </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>verified.injured_cum         </td> <td>29.04          </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>verified.killed_children     </td> <td>26.86          </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>verified.killed_children_cum </td> <td>26.68          </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>verified.injured_children    </td> <td>29.4           </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>verified.injured_children_cum</td> <td>29.04          </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- CHECKING QUALITY OF INFRASTRUCTURE DAMAGE ----------\n",
            "Total rows: 534\n",
            "\n",
            "Checking column: report_date\n",
            "\n",
            "Checking column: civic_buildings_destroyed\n",
            "\n",
            "Checking column: civic_buildings_ext_destroyed\n",
            "\n",
            "Checking column: educational_buildings_damaged\n",
            "\n",
            "Checking column: educational_buildings_destroyed\n",
            "\n",
            "Checking column: educational_buildings_ext_damaged\n",
            "\n",
            "Checking column: educational_buildings_ext_destroyed\n",
            "\n",
            "Checking column: places_of_worship_churches_destroyed\n",
            "\n",
            "Checking column: places_of_worship_ext_churches_destroyed\n",
            "\n",
            "Checking column: places_of_worship_ext_mosques_damaged\n",
            "\n",
            "Checking column: places_of_worship_ext_mosques_destroyed\n",
            "\n",
            "Checking column: places_of_worship_mosques_damaged\n",
            "\n",
            "Checking column: places_of_worship_mosques_destroyed\n",
            "\n",
            "Checking column: residential_destroyed\n",
            "\n",
            "Checking column: residential_ext_destroyed\n",
            "\n",
            "No missing values found in any columns!\n",
            "\n",
            "---------- CHECKING QUALITY OF KILLED IN GAZA ----------\n",
            "Total rows: 50020\n",
            "\n",
            "Checking column: Index\n",
            "\n",
            "Checking column: Name\n",
            "\n",
            "Checking column: مس#ا\n",
            "\n",
            "Checking column: Born\n",
            "\n",
            "Checking column: Age\n",
            "\n",
            "Checking column: Sex\n",
            "\n",
            "Checking column: ID number\n",
            "\n",
            "No missing values found in any columns!\n",
            "\n",
            "---------- OVERALL DATASET SUMMARY ----------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>Dataset</th> <th>Rows</th> <th>Columns</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>Gaza Casualties      </td> <td>551  </td> <td>18     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>West Bank Casualties </td> <td>551  </td> <td>15     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Infrastructure Damage</td> <td>534  </td> <td>15     </td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>Killed in Gaza       </td> <td>50020</td> <td>7      </td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def check_all_datasets_quality():\n",
        "\n",
        "    gaza_results = check_dataset_quality(daily_casualties_gaza, \"GAZA CASUALTIES\")\n",
        "    westbank_results = check_dataset_quality(daily_casualties_westbank, \"WEST BANK CASUALTIES\")\n",
        "    infra_results = check_dataset_quality(infrastructure_damaged, \"INFRASTRUCTURE DAMAGE\")\n",
        "    killed_results = check_dataset_quality(killed_in_gaza, \"KILLED IN GAZA\")\n",
        "\n",
        "    all_datasets = Table().with_columns(\n",
        "        'Dataset', ['Gaza Casualties', 'West Bank Casualties', 'Infrastructure Damage', 'Killed in Gaza'],\n",
        "        'Rows', [\n",
        "            daily_casualties_gaza.num_rows,\n",
        "            daily_casualties_westbank.num_rows,\n",
        "            infrastructure_damaged.num_rows,\n",
        "            killed_in_gaza.num_rows\n",
        "        ],\n",
        "        'Columns', [\n",
        "            len(daily_casualties_gaza.labels),\n",
        "            len(daily_casualties_westbank.labels),\n",
        "            len(infrastructure_damaged.labels),\n",
        "            len(killed_in_gaza.labels)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"\\n---------- OVERALL DATASET SUMMARY ----------\")\n",
        "    all_datasets.show()\n",
        "\n",
        "    return gaza_results, westbank_results, infra_results, killed_results\n",
        "\n",
        "gaza_results, westbank_results, infra_results, killed_results = check_all_datasets_quality()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gaza Casualties Dataset:**\n",
        "\n",
        "Columns with low missing rates (1-5%): killed, killed_cum, injured_cum\n",
        "Columns with high missing rates (~75%): med_killed_cum, press_killed_cum\n",
        "\n",
        "**West Bank Casualties Dataset:**\n",
        "\n",
        "Verified data columns missing ~27-29% of values\n",
        "Non-verified columns have complete data\n",
        "\n",
        "**Infrastructure Damage and Killed in Gaza datasets:**\n",
        "\n",
        "No missing values detected - these datasets are complete\n",
        "\n",
        "**-----------------------------------------------------**\n",
        "\n",
        "These patterns make sense when we understand how the data is collected. According to the dataset documentation:\n",
        "\n",
        "Medical personnel and journalist casualties in Gaza are updated less frequently (weekly) than other casualty numbers\n",
        "West Bank casualties go through a verification process by UN personnel, which creates delays in reporting verified data\n",
        "The \"verified\" columns in the West Bank dataset represent casualties independently confirmed by UN OCHA personnel\n"
      ],
      "metadata": {
        "id": "0UgGSVJ2hiAn"
      },
      "id": "0UgGSVJ2hiAn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Data Cleaning Process\n",
        "\n",
        "Based on our quality assessment and the dataset documentation, we'll implement cleaning strategies for each dataset with missing values while maintaining their integrity.\n"
      ],
      "metadata": {
        "id": "uyI3cl7Ig-Iz"
      },
      "id": "uyI3cl7Ig-Iz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 Cleaning Approach for Gaza Casualties Dataset\n",
        "\n",
        "**Approach 1: Columns Updated Weekly**\n",
        "\n",
        "For columns updated weekly (`med_killed_cum`, `press_killed_cum`):\n",
        "* Apply forward-filling to carry the last known value forward\n",
        "* aligns with the documentation which states: \"If there's no updated value for the given report date, the last reported value is used\"\n",
        "\n",
        "**Approach 2: Columns with Few Missing Values**\n",
        "\n",
        "For columns with few missing values (`killed`, `killed_cum`, `injured_cum`):\n",
        "* Use the corresponding extrapolated (`ext_`) columns when original values are missing\n",
        "* These `ext_` columns were created by the data providers specifically to fill gaps in the data\n"
      ],
      "metadata": {
        "id": "gNWiG9rqk2e4"
      },
      "id": "gNWiG9rqk2e4"
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_gaza_casualties(gaza_table):\n",
        "    \"\"\"\n",
        "    cleans the Gaza casualties dataset by:\n",
        "    1. forward filling the medical and press casualties columns (carrying forward the last observed value to fill in the gaps)\n",
        "    2. using ext_ columns when available for other missing data\n",
        "    \"\"\"\n",
        "    #a copy of the original table\n",
        "    cleaned = gaza_table.copy()\n",
        "\n",
        "    #forward fill\n",
        "    # (med_killed_cum and press_killed_cum)\n",
        "\n",
        "    # Helper function to forward fill a column\n",
        "    def forward_fill_column(table, column_name):\n",
        "        values = table.column(column_name)\n",
        "        filled_values = make_array()\n",
        "        last_valid = None\n",
        "\n",
        "        for value in values:\n",
        "            #check missing value\n",
        "            is_missing = False\n",
        "            try:\n",
        "                if value is None or (isinstance(value, float) and np.isnan(value)):\n",
        "                    is_missing = True\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            #if missing, use last valid value\n",
        "            if is_missing and last_valid is not None:\n",
        "                filled_values = np.append(filled_values, last_valid)\n",
        "            else:\n",
        "                filled_values = np.append(filled_values, value)\n",
        "                if not is_missing:\n",
        "                    last_valid = value\n",
        "\n",
        "        return filled_values\n",
        "\n",
        "    #forward-fill medical personnel casualties (med_killed_cum) because\n",
        "    #this counts medical workers killed which is updated weekly, so use last value for gaps\n",
        "    if 'med_killed_cum' in cleaned.labels:\n",
        "        filled_med = forward_fill_column(cleaned, 'med_killed_cum')\n",
        "        cleaned = cleaned.with_column('med_killed_cum', filled_med)\n",
        "\n",
        "    #forward-fill journalist casualties (press_killed_cum) because\n",
        "    #this counts journalists killed which is updated weekly, so use last value for gaps\n",
        "    if 'press_killed_cum' in cleaned.labels:\n",
        "        filled_press = forward_fill_column(cleaned, 'press_killed_cum')\n",
        "        cleaned = cleaned.with_column('press_killed_cum', filled_press)\n",
        "\n",
        "    #killed, killed_cum, injured_cum use the ext_ versions if original has missing value\n",
        "    # Helper function to fill using ext_ column when available\n",
        "    def fill_from_ext_column(table, column_name):\n",
        "        values = table.column(column_name)\n",
        "        ext_column = f'ext_{column_name}'\n",
        "        ext_values = table.column(ext_column)\n",
        "        filled_values = make_array()\n",
        "\n",
        "        for i in range(len(values)):\n",
        "            #check value is missing\n",
        "            is_missing = False\n",
        "            try:\n",
        "                if values.item(i) is None or (isinstance(values.item(i), float) and np.isnan(values.item(i))):\n",
        "                    is_missing = True\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            #if missing, use expt value\n",
        "            if is_missing:\n",
        "                filled_values = np.append(filled_values, ext_values.item(i))\n",
        "            else:\n",
        "                filled_values = np.append(filled_values, values.item(i))\n",
        "\n",
        "        return filled_values\n",
        "\n",
        "    #fill killed using ext\n",
        "    if 'killed' in cleaned.labels and 'ext_killed' in cleaned.labels:\n",
        "        filled_killed = fill_from_ext_column(cleaned, 'killed')\n",
        "        cleaned = cleaned.with_column('killed', filled_killed)\n",
        "\n",
        "    #fill cumulative killed count using ext\n",
        "    if 'killed_cum' in cleaned.labels and 'ext_killed_cum' in cleaned.labels:\n",
        "        filled_killed_cum = fill_from_ext_column(cleaned, 'killed_cum')\n",
        "        cleaned = cleaned.with_column('killed_cum', filled_killed_cum)\n",
        "\n",
        "    #fill cumulative injured count using ext\n",
        "    if 'injured_cum' in cleaned.labels and 'ext_injured_cum' in cleaned.labels:\n",
        "        filled_injured_cum = fill_from_ext_column(cleaned, 'injured_cum')\n",
        "        cleaned = cleaned.with_column('injured_cum', filled_injured_cum)\n",
        "\n",
        "    return cleaned"
      ],
      "metadata": {
        "id": "jAWbbGe0knm2"
      },
      "id": "jAWbbGe0knm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 Cleaning Approach for West Bank Dataset\n",
        "\n",
        "For the West Bank dataset, we'll create two  datasets:\n",
        "\n",
        "**Dataset 1: Verified-Only Table**\n",
        "\n",
        "* Preserves only UN-verified data with its gaps\n",
        "* This dataset has the high reliability but contains gaps in the data\n",
        "\n",
        "**Dataset 2: Complete Table**\n",
        "\n",
        "* Uses non-verified data to fill gaps when verified data is missing\n",
        "* This dataset provides a complete timeline but with less verification\n",
        "\n",
        "**This approach gives us flexibility:**\n",
        "* When accuracy is needed, we can use the verified-only dataset\n",
        "* When analyzing trends over time, we can use the complete dataset"
      ],
      "metadata": {
        "id": "9JEQoV3vlDB_"
      },
      "id": "9JEQoV3vlDB_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3718248-d3b0-47dd-9b50-41975f92d1a2",
      "metadata": {
        "id": "b3718248-d3b0-47dd-9b50-41975f92d1a2",
        "outputId": "cf1cf54f-e893-45f9-91ed-51fd360f0440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Checking missing values in Clean Gaza Casualties:\n",
            "check complete for Clean Gaza Casualties\n",
            "\n",
            "Checking missing values in West Bank Verified Data:\n",
            "  verified.killed_cum: 147 missing values (26.68%)\n",
            "  verified.injured_cum: 160 missing values (29.04%)\n",
            "  verified.killed_children_cum: 147 missing values (26.68%)\n",
            "  verified.injured_children_cum: 160 missing values (29.04%)\n",
            "check complete for West Bank Verified Data\n",
            "\n",
            "Checking missing values in West Bank Complete Data:\n",
            "check complete for West Bank Complete Data\n"
          ]
        }
      ],
      "source": [
        "def prepare_west_bank_data(wb_table):\n",
        "    \"\"\"\n",
        "    Creates two tables from West Bank data:\n",
        "    1. verified_only: contains only verified casualty data (for trustworthy analysis)\n",
        "    2. complete: uses non-verified data to fill gaps (for timeline analysis)\n",
        "    \"\"\"\n",
        "    #create a table with only verified data (has gaps but is confirmed)\n",
        "    verified_only = wb_table.select(\n",
        "        'report_date',\n",
        "        'verified.killed_cum',\n",
        "        'verified.injured_cum',\n",
        "        'verified.killed_children_cum',\n",
        "        'verified.injured_children_cum'\n",
        "    )\n",
        "\n",
        "    #create a complete table using non-verified data (fewer gaps) to replaces missing verified data with corresponding non-verified column\n",
        "    #helper function to combine verified and non-verified columns\n",
        "    def combine_verified_with_fallback(verified_col, fallback_col):\n",
        "        combined = make_array()\n",
        "\n",
        "        for i in range(len(verified_col)):\n",
        "            is_missing = False\n",
        "            try:\n",
        "                if verified_col.item(i) is None or (isinstance(verified_col.item(i), float) and np.isnan(verified_col.item(i))):\n",
        "                    is_missing = True\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            #if missing use the non verified value as fallback\n",
        "            if is_missing:\n",
        "                combined = np.append(combined, fallback_col.item(i))\n",
        "            else:\n",
        "                combined = np.append(combined, verified_col.item(i))\n",
        "\n",
        "        return combined\n",
        "\n",
        "    #create complete table with combined data\n",
        "    complete = Table().with_column('report_date', wb_table.column('report_date'))\n",
        "\n",
        "    #add combined columns (verified when available, otherwise use non-verified)\n",
        "    if 'verified.killed_cum' in wb_table.labels and 'killed_cum' in wb_table.labels:\n",
        "        killed = combine_verified_with_fallback(\n",
        "            wb_table.column('verified.killed_cum'),\n",
        "            wb_table.column('killed_cum')\n",
        "        )\n",
        "        complete = complete.with_column('killed_cum', killed)\n",
        "\n",
        "    if 'verified.injured_cum' in wb_table.labels and 'injured_cum' in wb_table.labels:\n",
        "        injured = combine_verified_with_fallback(\n",
        "            wb_table.column('verified.injured_cum'),\n",
        "            wb_table.column('injured_cum')\n",
        "        )\n",
        "        complete = complete.with_column('injured_cum', injured)\n",
        "\n",
        "    if 'verified.killed_children_cum' in wb_table.labels and 'killed_children_cum' in wb_table.labels:\n",
        "        killed_children = combine_verified_with_fallback(\n",
        "            wb_table.column('verified.killed_children_cum'),\n",
        "            wb_table.column('killed_children_cum')\n",
        "        )\n",
        "        complete = complete.with_column('killed_children_cum', killed_children)\n",
        "\n",
        "    if 'verified.injured_children_cum' in wb_table.labels and 'injured_children_cum' in wb_table.labels:\n",
        "        injured_children = combine_verified_with_fallback(\n",
        "            wb_table.column('verified.injured_children_cum'),\n",
        "            wb_table.column('injured_children_cum')\n",
        "        )\n",
        "        complete = complete.with_column('injured_children_cum', injured_children)\n",
        "\n",
        "    return verified_only, complete\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.3 Implementing the Data Cleaning\n",
        "We apply our cleaning function them to our datasets and verify that they successfully address the missing values."
      ],
      "metadata": {
        "id": "F6GSwg9GlcGL"
      },
      "id": "F6GSwg9GlcGL"
    },
    {
      "cell_type": "code",
      "source": [
        "#clean the datasets\n",
        "gaza_clean = clean_gaza_casualties(daily_casualties_gaza)\n",
        "wb_verified, wb_complete = prepare_west_bank_data(daily_casualties_westbank)\n",
        "\n",
        "#verification\n",
        "def check_missing_values(table, name):\n",
        "    \"\"\"Check if any missing values remain in key columns\"\"\"\n",
        "    print(f\"\\nChecking missing values in {name}:\")\n",
        "\n",
        "    for col in table.labels:\n",
        "        #skip checking date columns and non numeric columns\n",
        "        if 'date' in col or col == 'report_source' or col == 'flash_source':\n",
        "            continue\n",
        "\n",
        "        values = table.column(col)\n",
        "        missing_count = 0\n",
        "\n",
        "        for value in values:\n",
        "            try:\n",
        "                if value is None or (isinstance(value, float) and np.isnan(value)):\n",
        "                    missing_count += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if missing_count > 0:\n",
        "            print(f\"  {col}: {missing_count} missing values ({missing_count/table.num_rows*100:.2f}%)\")\n",
        "\n",
        "    print(f\"check complete for {name}\")\n",
        "\n",
        "#verify\n",
        "check_missing_values(gaza_clean, \"Clean Gaza Casualties\")\n",
        "check_missing_values(wb_verified, \"West Bank Verified Data\")\n",
        "check_missing_values(wb_complete, \"West Bank Complete Data\")"
      ],
      "metadata": {
        "id": "g2dTC_6TksWw"
      },
      "id": "g2dTC_6TksWw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fabfe247-e995-4d80-a5e1-8d290e0c6235",
      "metadata": {
        "id": "fabfe247-e995-4d80-a5e1-8d290e0c6235"
      },
      "source": [
        "## 2. Exploratory Analysis & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01373f44-d1a2-4895-9b68-328681c1a98f",
      "metadata": {
        "id": "01373f44-d1a2-4895-9b68-328681c1a98f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39212e78-214e-4ae0-89bc-7559ea9911d7",
      "metadata": {
        "id": "39212e78-214e-4ae0-89bc-7559ea9911d7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3336077-0e3d-4690-9480-79098117fa00",
      "metadata": {
        "id": "e3336077-0e3d-4690-9480-79098117fa00"
      },
      "outputs": [],
      "source": [
        "# casualities over time for gaza, westbank (overlaid line plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f328092-6813-488e-97e8-d6b19aa1decf",
      "metadata": {
        "id": "6f328092-6813-488e-97e8-d6b19aa1decf"
      },
      "outputs": [],
      "source": [
        "# age groups (in ranges) histogram (killed in gaza)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed5f66e1-7f00-41ba-b76e-20a6bbb47e32",
      "metadata": {
        "id": "ed5f66e1-7f00-41ba-b76e-20a6bbb47e32"
      },
      "outputs": [],
      "source": [
        "# join by report_date, number of building vs num killed in gaza (scatter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4270c896-7a12-40cd-ad70-cf85dd6e7c39",
      "metadata": {
        "id": "4270c896-7a12-40cd-ad70-cf85dd6e7c39"
      },
      "outputs": [],
      "source": [
        "# group by gender (killed in gaza) bar chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c5adde-549b-4115-88d7-af28e821cdbc",
      "metadata": {
        "id": "03c5adde-549b-4115-88d7-af28e821cdbc"
      },
      "outputs": [],
      "source": [
        "# something for period of ceasefire vs normal casualities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc0ddcb-a239-41dd-8688-cb869a1b3862",
      "metadata": {
        "id": "efc0ddcb-a239-41dd-8688-cb869a1b3862"
      },
      "outputs": [],
      "source": [
        "# select just medical killed column vs time (and press)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd568a3-fc93-410e-8397-e50cba01cf2b",
      "metadata": {
        "id": "abd568a3-fc93-410e-8397-e50cba01cf2b"
      },
      "outputs": [],
      "source": [
        "# settler attacks vs casualities over time in westband"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaea4c4a-7eaf-4433-bc07-817397dc52b1",
      "metadata": {
        "id": "eaea4c4a-7eaf-4433-bc07-817397dc52b1"
      },
      "source": [
        "## 3. Hypothesis Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "656ecd97-0c78-4b32-a606-ac7d72a525f0",
      "metadata": {
        "id": "656ecd97-0c78-4b32-a606-ac7d72a525f0"
      },
      "outputs": [],
      "source": [
        "# null: woman and men are being targetted equally\n",
        "# alternative: woman are being targetted more/ children\n",
        "# statistic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "842199cc-bd3e-4f7a-aaaa-0a3cc73d2511",
      "metadata": {
        "id": "842199cc-bd3e-4f7a-aaaa-0a3cc73d2511"
      },
      "outputs": [],
      "source": [
        "# null: all ages are being targetted equally\n",
        "# alternative: younger people (under a certain age)\n",
        "# use td"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ea957d-032b-4c9c-8676-6ddd1c4c7c71",
      "metadata": {
        "id": "b0ea957d-032b-4c9c-8676-6ddd1c4c7c71"
      },
      "outputs": [],
      "source": [
        "#\n",
        "#\n",
        "#"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}